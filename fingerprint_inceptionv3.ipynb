{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fingerprint_inceptionv3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishtrivedi16/major_project_stuff/blob/master/fingerprint_inceptionv3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_OlmI4V-KJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GPU details\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvuGD9jd_LgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CPU details\n",
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLTDdklcEsY3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Mount your gdrive account by running this cell and place your database files\n",
        "in it for easy access.\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1keaSRqEv9E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os # for basic operations like folder creation, directory validation etc\n",
        "import glob # finds file according to wildcard given\n",
        "import shutil # for moving files\n",
        "import matplotlib.pyplot as plt # for plotting graphs and viewing images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YD01ByVaUTIm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "This function organises fingerprint of a person to his respective folder\n",
        "No need to run this cell as i've already organised the images into their\n",
        "respective folders\n",
        "'''\n",
        "\n",
        "src = \"./drive/My Drive/dbs/DB1_B/dataset\"\n",
        "des = \"./drive/My Drive/dbs/DB1_B/dataset\"\n",
        "def organise_files(path=src):\n",
        "    for i in range(101, 111):\n",
        "        if not os.path.isdir(src + str(i)):\n",
        "            os.makedirs(src + str(i))\n",
        "        for name in glob.glob(src + str(i) + \"_*.tif\"):\n",
        "            shutil.move(name, des + str(i) )\n",
        "\n",
        "organise_files()            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqJC-S2zTz4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Visualise file directory structure\n",
        "'''\n",
        "\n",
        "startpath = './drive/My Drive/dbs/DB1_B'\n",
        "\n",
        "def list_files(startpath):\n",
        "    for root, dirs, files in os.walk(startpath):\n",
        "        level = root.replace(startpath, '').count(os.sep)\n",
        "        indent = ' ' * 4 * (level)\n",
        "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
        "        subindent = ' ' * 4 * (level + 1)\n",
        "        for f in files:\n",
        "            print('{}{}'.format(subindent, f))\n",
        "\n",
        "list_files(startpath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuLK7qlpVMLZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "717e3334-8580-45e7-d532-56aed5aa5fc0"
      },
      "source": [
        "'''\n",
        "function to divide the dataset into train and test folders for \n",
        "ImageDataGenerator.flow_from_directory() function\n",
        "\n",
        "train set -> 80% (6 fingerprints per person)\n",
        "validation set -> 20% (2 fingerprints per person)\n",
        "'''\n",
        "\n",
        "startpath = './drive/My Drive/dbs/DB1_B/'\n",
        "train_split = 0.80\n",
        "validation_split = 0.20\n",
        "\n",
        "def train_validation_split(startpath, train=train_split, validation=validation_split):\n",
        "    # first create same sub-diretory structure in both train and validation folders\n",
        "    dir_list  = [x[1] for x in os.walk(startpath + 'dataset')][0]\n",
        "    for dir in dir_list:\n",
        "        if not os.path.isdir(startpath + 'train/' + dir):\n",
        "            os.mkdir(startpath + 'train/' + dir)\n",
        "        if not os.path.isdir(startpath + 'validation/' + dir):\n",
        "            os.mkdir(startpath + 'validation/' + dir)\n",
        "\n",
        "    # randomly select images based on train validation slipt and move them\n",
        "    # to their respective folders\n",
        "    filename_list = [x[0] for x in os.walk(startpath + 'dataset')][1:]\n",
        "    print(filename_list)\n",
        "\n",
        "\n",
        "train_validation_split(startpath)\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['./drive/My Drive/dbs/DB1_B/dataset/110', './drive/My Drive/dbs/DB1_B/dataset/105', './drive/My Drive/dbs/DB1_B/dataset/103', './drive/My Drive/dbs/DB1_B/dataset/102', './drive/My Drive/dbs/DB1_B/dataset/104', './drive/My Drive/dbs/DB1_B/dataset/108', './drive/My Drive/dbs/DB1_B/dataset/106', './drive/My Drive/dbs/DB1_B/dataset/107', './drive/My Drive/dbs/DB1_B/dataset/101', './drive/My Drive/dbs/DB1_B/dataset/109']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeOpOs-iPepd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "-> Generator class for applying image transformations\n",
        "-> Can be used for image augmentation purposes if need be\n",
        "-> Currently just normalises the image\n",
        "-> tensorflow does not support importing and processing images\n",
        "    so use this or use PIL(python image library)\n",
        "'''\n",
        "from keras.preprocessing.image import ImageDataGenerator \n",
        "\n",
        "# Image dimension data\n",
        "'''\n",
        "    Sensor Type\t    Image Size\t            Set A\tSet B\tResolution\n",
        "DB1\tOptical Sensor\t640x480 (307 Kpixels)\t100x8\t10x8\t500 dpi\n",
        "DB2\tOptical Sensor\t328x364 (119 Kpixels)\t100x8\t10x8\t500 dpi\n",
        "DB3\tThermal sweeping\n",
        "    Sensor\t        300x480 (144 Kpixels)\t100x8\t10x8\t512 dpi\n",
        "DB4\tSFinGe v3.0\t    288x384 (108 Kpixels)\t100x8\t10x8\tabout 500 dpi\n",
        "'''\n",
        "\n",
        "img_width, img_height = 150, 150\n",
        "train_data_dir = './drive/My Drive/dbs/DB1_B/train'\n",
        "validation_data_dir = './drive/My Drive/dbs/DB1_B/validation'\n",
        "nb_train_samples = 2000\n",
        "nb_validation_samples = 800\n",
        "batch_size = 10\n",
        "epochs = 150\n",
        "\n",
        "if K.image_data_format() == 'channels_last':\n",
        "    input_tensor = Input(shape=(img_width, img_height, 3))\n",
        "else:\n",
        "    input_tensor = Input(shape=(3, img_width, img_height))\n",
        "\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        './drive/My Drive/dbs/DB1_B/',\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        'data/validation',\n",
        "        target_size=(img_width, img_height),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical')\n",
        "\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=2000,\n",
        "        epochs=50,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=800)\n",
        "\n",
        "from keras.models import Model\n",
        "from keras import backend as K\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "# imports the inceptionv3 pretained model\n",
        "inception_model = InceptionV3(input_tensor=input_tensor, weights='imagenet', include_top=False)\n",
        "\n",
        "x = inception_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(10, activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=inception_model.input, outputs=predictions)\n",
        "\n",
        "# freeze all convolutional InceptionV3 layers\n",
        "for layer in inception_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
        "# model.summary()\n",
        "\n",
        "model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=nb_train_samples // batch_size,\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=nb_validation_samples // batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}